
output exec {{

    inline uint64_t
    sew_mask(const uint64_t vtype)
    {
        return (1 << (3 + bits(vtype, 5, 3))) - 1;
    }

    inline uint64_t
    SEW(const uint64_t vtype)
    {
        return 8 << bits(vtype, 5, 3);
    }

    /*
    * Encode LMUL to lmul as follows:
    *     LMUL    vlmul    lmul
    *      1       000       0
    *      2       001       1
    *      4       010       2
    *      8       011       3
    *      -       100       -
    *     1/8      101      -3
    *     1/4      110      -2
    *     1/2      111      -1
    *
    * then, we can calculate VLMAX = vlen >> (vsew + 3 - lmul)
    * e.g. vlen = 256 bits, SEW = 16, LMUL = 1/8
    *      => VLMAX = vlen >> (1 + 3 - (-3))
    *               = 256 >> 7
    *               = 2
    * Ref: https://github.com/qemu/qemu/blob/5e9d14f2/target/riscv/cpu.h
    */
    inline uint64_t
    VLMAX(const uint64_t vtype, const bool per_reg = false)
    {
        int64_t lmul = (int64_t)sext<3>(bits(vtype, 2, 0));
        lmul = per_reg ? std::min<int64_t>(0, lmul) : lmul;
        int64_t vsew = bits(vtype, 5, 3);
        return TheISA::VLEN >> (vsew + 3 - lmul);
    }

    inline uint64_t
    reg_group_nums(const uint64_t vtype)
    {
        int64_t lmul = (int64_t)sext<3>(bits(vtype, 2, 0));
        return 1 << std::max<int64_t>(0, lmul);
    }

    inline void
    set_vill(uint64_t& vtype)
    {
        vtype = (uint64_t)0 ^ (1UL << (XLEN-1));
    }

    inline size_t
    mem_bits(const uint8_t width)
    {
        return width == 0 ? 8 : (1 << (width - 1));
    }

    template<typename Type>
    bool inline
    carry_out(Type a, Type b, bool carry_in = false) {
        using TypeU = std::make_unsigned_t<Type>;
        TypeU s = *reinterpret_cast<TypeU*>(&a)
                + *reinterpret_cast<TypeU*>(&b) + carry_in;
        return carry_in
            ? (s <= *reinterpret_cast<TypeU*>(&a))
            : (s <  *reinterpret_cast<TypeU*>(&a));
    }

    template<typename Type>
    bool inline
    borrow_out(Type a, Type b, bool borrow_in = false) {
        using TypeU = std::make_unsigned_t<Type>;
        return borrow_in
            ? (*reinterpret_cast<TypeU*>(&a) <= *reinterpret_cast<TypeU*>(&b))
            : (*reinterpret_cast<TypeU*>(&a) <  *reinterpret_cast<TypeU*>(&b));
    }

    inline uint32_t
    fsgnj32(uint32_t a, uint32_t b, bool n, bool x) {
        if (n) b = ~b;
        else if (x) b = a ^ b;
        return insertBits(b, 30, 0, a);
    }

    inline uint64_t
    fsgnj64(uint64_t a, uint64_t b, bool n, bool x) {
        if (n) b = ~b;
        else if (x) b = a ^ b;
        return insertBits(b, 62, 0, a);
    }

    #define CHECK_VILL                                                      \
    if (bits(tc->readMiscReg(MISCREG_VTYPE), XLEN - 1))                     \
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);

    #define ASSIGN_VD_BIT(idx, bit) \
    ((Vds[(idx)/8] & ~(1 << (idx)%8)) | ((bit) << (idx)%8))

    #define VLOOP(...)                              \
    for (; ei < vl; ei++) {                         \
        size_t i = ei - vlmax_per_reg * greg_idx;   \
        if (i >= vlmax_per_reg) break;              \
        __VA_ARGS__                                 \
    }

    #define VLOOP_IF_MASK(...)                      \
    for (; ei < vl; ei++) {                         \
        size_t i = ei - vlmax_per_reg * greg_idx;   \
        if (i >= vlmax_per_reg) break;              \
        if(vm || elem_mask(v0, ei)) __VA_ARGS__     \
    }
}};

def format VConfOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VConfOp', code, flags)
    header_output = BasicDeclare.subst(iop)
    decoder_output = BasicConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VConfExecute.subst(iop)
}};

def template VConfExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        %(op_decl)s;
        %(op_rd)s;
        %(code)s;

        if (vlmax == 0) {
            set_vill(vtype);
            vl = 0;
        }
        tc->setMiscReg(MISCREG_VL, vl);
        tc->setMiscReg(MISCREG_VTYPE, vtype);
        tc->setMiscReg(MISCREG_VSTART, 0);
        tc->getDecoderPtr()->as<Decoder>().setVl(vl);
        tc->getDecoderPtr()->as<Decoder>().setVtype(vtype);
        %(op_wb)s;
        return NoFault;
    }
}};

def format VIntOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VArithOp', code, flags)
    header_output = BasicDeclare.subst(iop)
    decoder_output = BasicConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VIntExecute.subst(iop)
}};

def template VIntExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        uint64_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();
        auto nums = reg_group_nums(vtype);
        [[maybe_unused]] auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;

        bool break_loop = false;
        for (; greg_idx < nums && !break_loop; greg_idx++) {
            if (ei >= vl) break;
            switch (SEW(vtype)) {
                case 8: {
                    using vi [[maybe_unused]] = int8_t;
                    using vu [[maybe_unused]] = uint8_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 16: {
                    using vi [[maybe_unused]] = int16_t;
                    using vu [[maybe_unused]] = uint16_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 32: {
                    using vi [[maybe_unused]] = int32_t;
                    using vu [[maybe_unused]] = uint32_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 64: {
                    using vi [[maybe_unused]] = int64_t;
                    using vu [[maybe_unused]] = uint64_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                default:
                    panic("Invalid SEW");
            }
        }

        return NoFault;
    }
}};

def template VArithMaskDeclare {{
    class %(class_name)s : public %(base_class)s
    {
      private:
        // Add dest reg manually for VArithMask insts
        RegId srcRegIdxArr[2];
        RegId destRegIdxArr[1];

      public:
        %(class_name)s(ExtMachInst machInst);
        Fault execute(ExecContext *, Trace::InstRecord *) const override;
        using %(base_class)s::generateDisassembly;
    };
}};

def template VArithMaskConstructor {{
    %(class_name)s::%(class_name)s(ExtMachInst machInst):
        %(base_class)s("%(mnemonic)s", machInst, %(op_class)s)
    {
        %(set_reg_idx_arr)s;
        %(constructor)s;
        // Add dest reg manually for VArithMask insts
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, VD));
        _numVecDestRegs++;
    };
}};

def format VIntMaskOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VArithOp', code, flags)
    header_output = VArithMaskDeclare.subst(iop)
    decoder_output = VArithMaskConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VIntMaskExecute.subst(iop)
}};

def template VIntMaskExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        uint64_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();
        auto nums = reg_group_nums(vtype);
        [[maybe_unused]] auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;

        TheISA::VecRegContainer tmp_d0 =
            xc->getWritableVecRegOperand(this, 0);
        auto Vds = tmp_d0.as<uint8_t>();

        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            switch (SEW(vtype)) {
                case 8: {
                    using vi [[maybe_unused]] = int8_t;
                    using vu [[maybe_unused]] = uint8_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 16: {
                    using vi [[maybe_unused]] = int16_t;
                    using vu [[maybe_unused]] = uint16_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 32: {
                    using vi [[maybe_unused]] = int32_t;
                    using vu [[maybe_unused]] = uint32_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                case 64: {
                    using vi [[maybe_unused]] = int64_t;
                    using vu [[maybe_unused]] = uint64_t;
                    %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
                    break;
                }
                default:
                    panic("Invalid SEW");
            }
        }
        xc->setVecRegOperand(this, 0, tmp_d0);
        if (traceData) {
            traceData->setData(tmp_d0);
        }
        return NoFault;
    }
}};

def format VFloatOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VArithOp', code, flags)
    header_output = BasicDeclare.subst(iop)
    decoder_output = BasicConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VFloatExecute.subst(iop)
}};

def template VFloatExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        STATUS status = xc->readMiscReg(MISCREG_STATUS);
        if (status.fs == FPUStatus::OFF)
            return std::make_shared<IllegalInstFault>("VFPU is off",
                                                        machInst);

        VRM_REQUIRED;

        uint64_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;

        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            switch (SEW(vtype)) {
                case 32: {
                    using vi [[maybe_unused]] = int32_t;
                    using vu = uint32_t;
                    [[maybe_unused]] auto ftype = [&](vu v){return f32(v);};
                    [[maybe_unused]] auto fsqrt = f32_sqrt;
                    [[maybe_unused]] auto fmask = mask(31, 31);
                    [[maybe_unused]] auto
                        fadd = f32_add,
                        fsub = f32_sub,
                        fmin = f32_min,
                        fmax = f32_max,
                        fdiv = f32_div,
                        fmul = f32_mul;
                    [[maybe_unused]] auto fmadd = f32_mulAdd;
                    [[maybe_unused]] auto fsgnj = fsgnj32;
                    [[maybe_unused]] auto f_to_ui = f32_to_ui32;
                    [[maybe_unused]] auto f_to_i =  f32_to_i32;
                    [[maybe_unused]] auto ui_to_f = ui32_to_f32;
                    [[maybe_unused]] auto i_to_f =  i32_to_f32;
                    [[maybe_unused]] auto unbox = unboxF32;

                    %(op_decl)s; %(op_rd)s;

                    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
                    std::feclearexcept(FE_ALL_EXCEPT);

                    %(code)s;

                    FFLAGS |= softfloat_exceptionFlags;
                    softfloat_exceptionFlags = 0;
                    xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);

                    %(op_wb)s;
                    break;
                }
                case 64: {
                    using vi [[maybe_unused]] = int64_t;
                    using vu = uint64_t;
                    [[maybe_unused]] auto ftype = [&](vu v){return f64(v);};
                    [[maybe_unused]] auto fsqrt = f64_sqrt;
                    [[maybe_unused]] auto fmask = mask(63, 63);
                    [[maybe_unused]] auto
                        fadd = f64_add,
                        fsub = f64_sub,
                        fmin = f64_min,
                        fmax = f64_max,
                        fdiv = f64_div,
                        fmul = f64_mul;
                    [[maybe_unused]] auto fmadd = f64_mulAdd;
                    [[maybe_unused]] auto fsgnj = fsgnj64;
                    [[maybe_unused]] auto f_to_ui = f64_to_ui64;
                    [[maybe_unused]] auto f_to_i =  f64_to_i64;
                    [[maybe_unused]] auto ui_to_f = ui64_to_f64;
                    [[maybe_unused]] auto i_to_f =  i64_to_f64;
                    [[maybe_unused]] auto unbox = [&](vu v) { return v; };

                    %(op_decl)s; %(op_rd)s;

                    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
                    std::feclearexcept(FE_ALL_EXCEPT);

                    %(code)s;

                    FFLAGS |= softfloat_exceptionFlags;
                    softfloat_exceptionFlags = 0;
                    xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);

                    %(op_wb)s;
                    break;
                }
                default:
                    auto message = "VFPU does not support float" +
                                    std::to_string(SEW(vtype));
                    return std::make_shared<IllegalInstFault>(message,
                        machInst);
            }
        }

        return NoFault;
    }
}};

def format VFloatMaskOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VArithOp', code, flags)
    header_output = VArithMaskDeclare.subst(iop)
    decoder_output = VArithMaskConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VFloatMaskExecute.subst(iop)
}};

def template VFloatMaskExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        STATUS status = xc->readMiscReg(MISCREG_STATUS);
        if (status.fs == FPUStatus::OFF)
            return std::make_shared<IllegalInstFault>("VFPU is off",
                                                        machInst);

        VRM_REQUIRED;

        uint64_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;

        TheISA::VecRegContainer tmp_d0 =
            xc->getWritableVecRegOperand(this, 0);
        auto Vds = tmp_d0.as<uint8_t>();

        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            switch (SEW(vtype)) {
                case 32: {
                    using vi [[maybe_unused]] = int32_t;
                    using vu = uint32_t;
                    std::function<float32_t(vu)> ftype =
                        [&](vu v){ return f32(v); };
                    std::function<bool(float32_t, float32_t)>
                        fle = f32_le,
                        flt = f32_lt,
                        feq = f32_eq;

                    %(op_decl)s; %(op_rd)s; %(op_wb)s;

                    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
                    std::feclearexcept(FE_ALL_EXCEPT);

                    %(code)s;

                    FFLAGS |= softfloat_exceptionFlags;
                    softfloat_exceptionFlags = 0;
                    xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);
                    break;
                }
                case 64: {
                    using vi [[maybe_unused]] = int64_t;
                    using vu = uint64_t;
                    std::function<float64_t(vu)> ftype =
                        [&](vu v){ return f64(v); };
                    std::function<bool(float64_t, float64_t)>
                        fle = f64_le,
                        flt = f64_lt,
                        feq = f64_eq;

                    %(op_decl)s; %(op_rd)s; %(op_wb)s;

                    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
                    std::feclearexcept(FE_ALL_EXCEPT);

                    %(code)s;

                    FFLAGS |= softfloat_exceptionFlags;
                    softfloat_exceptionFlags = 0;
                    xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);
                    break;
                }
                default:
                    auto message = "VFPU does not support float" +
                                    std::to_string(SEW(vtype));
                    return std::make_shared<IllegalInstFault>(message,
                        machInst);
            }
        }
        xc->setVecRegOperand(this, 0, tmp_d0);
        if (traceData) {
            traceData->setData(tmp_d0);
        }
        return NoFault;
    }
}};

def format VMvWholeOp(code, *flags) {{
    iop = InstObjParams(name, Name, 'VArithOp', code, flags)
    header_output = VMvWholeDeclare.subst(iop)
    decoder_output = BasicConstructor.subst(iop)
    decode_block = BasicDecode.subst(iop)
    exec_output = VMvWholeExecute.subst(iop)
}};

def template VMvWholeExecute {{
    Fault
    %(class_name)s::execute(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        // TODO: Check register align
        // TODO: If vd is equal to vs2 the instruction is an architectural NOP
        size_t NREG = SIMM_3 + 1;
        for (size_t greg_idx = 0; greg_idx < NREG; greg_idx++) {
            for (size_t i = 0; i < TheISA::NumVecElemPerVecReg; i++) {
                %(op_decl)s; %(op_rd)s; %(code)s; %(op_wb)s;
            }
        }
        return NoFault;
    }

    std::string
    %(class_name)s::generateDisassembly(
            Addr pc, const loader::SymbolTable *symtab) const
    {
        std::stringstream ss;
        ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", " <<
            registerName(srcRegIdx(1));
        return ss.str();
    }
}};

def template VMvWholeDeclare {{
    class %(class_name)s : public %(base_class)s
    {
      private:
        %(reg_idx_arr_decl)s;

      public:
        %(class_name)s(ExtMachInst extMachInst);
        Fault execute(ExecContext *, Trace::InstRecord *) const override;
        std::string generateDisassembly(
            Addr pc, const loader::SymbolTable *symtab) const override;
    };
}};


def template VMemLoadStoreOpDeclare {{
    /**
     * Static instruction class for "%(mnemonic)s".
     */
    class %(class_name)s : public %(base_class)s
    {
      private:
        %(reg_idx_arr_decl)s;

      public:
        /// Constructor.
        %(class_name)s(ExtMachInst extMachInst);

        Fault execute(ExecContext *, Trace::InstRecord *) const override;
        Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
        Fault completeAcc(PacketPtr, ExecContext *,
                          Trace::InstRecord *) const override;
    };
}};

def template VMemLoadStoreOpConstructor {{
    %(class_name)s::%(class_name)s(ExtMachInst extMachInst):
        %(base_class)s("%(mnemonic)s", extMachInst, %(op_class)s,
        (extMachInst.mew<<3)|extMachInst.width, extMachInst.vm)
    {
        %(set_reg_idx_arr)s;
        %(constructor)s;
    }
}};

let {{
    def VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
            mem_flags, inst_flags, base_class,
            decode_template=BasicDecode, exec_template_base=''):
        # Make sure flags are in lists (convert to lists if not).
        mem_flags = makeList(mem_flags)
        inst_flags = makeList(inst_flags)

        iop = InstObjParams(name, Name, base_class,
            {'ea_code': ea_code, 'memacc_code': memacc_code },
            inst_flags)

        if mem_flags:
            mem_flags = [ 'Request::%s' % flag for flag in mem_flags ]
            s = '\n\tthis->memAccessFlags = ' + '|'.join(mem_flags) + ';'
            iop.constructor += s

        # select templates
        fullExecTemplate = eval(exec_template_base + 'Execute')
        initiateAccTemplate = eval(exec_template_base + 'InitiateAcc')
        completeAccTemplate = eval(exec_template_base + 'CompleteAcc')

        # (header_output, decoder_output, decode_block, exec_output)
        return (VMemLoadStoreOpDeclare.subst(iop),
            VMemLoadStoreOpConstructor.subst(iop),
            decode_template.subst(iop),
            fullExecTemplate.subst(iop) +
            initiateAccTemplate.subst(iop) +
            completeAccTemplate.subst(iop))
}};

def format VMemLoadOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemLoadOp', exec_template_base='VMemLoad')
}};

def template VMemLoadExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        Addr EA;
        Fault fault = NoFault;

        size_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        STATUS status = xc->readMiscReg(MISCREG_STATUS);
        if (status.vs == static_cast<int>(VPUStatus::OFF))
            fault = std::make_shared<IllegalInstFault>("VPU is off", machInst);

        if (fault != NoFault) return fault;

        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;
        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            %(op_decl)s; %(op_rd)s; %(ea_code)s;

            auto offset = TheISA::vlenb * greg_idx;
            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            fault = readMemAtomic(xc, EA + offset, Mem.as<uint8_t>(),
                TheISA::vlenb, this->memAccessFlags, byte_enable);
            if (fault != NoFault) return fault;

            %(memacc_code)s;
            %(op_wb)s;
        }

        return fault;
    }
}};

def template VMemLoadInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        auto nums = reg_group_nums(vtype);
        Addr EA;

        %(op_src_decl)s;
        %(op_rd)s;
        %(ea_code)s;

        const std::vector<bool> byte_enable(sizeof(Mem) * nums, true);
        return initiateMemRead(xc, EA, sizeof(Mem) * nums,
            this->memAccessFlags, byte_enable);
    }
}};

def template VMemLoadCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        Fault fault = NoFault;

        auto tc = xc->tcBase();
        size_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);

        STATUS status = xc->readMiscReg(MISCREG_STATUS);
        if (status.vs == static_cast<int>(VPUStatus::OFF))
            fault = std::make_shared<IllegalInstFault>("VPU is off", machInst);

        if (fault != NoFault) return fault;

        const TheISA::vreg_t& tmp_v0 =
            tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;
        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            %(op_decl)s; %(op_rd)s;

            auto offset = TheISA::vlenb * greg_idx;
            memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>() + offset,
                   TheISA::vlenb);
            VLOOP({ %(memacc_code)s; });
            %(op_wb)s;
        }

        return fault;
    }
}};

def format VMemLoadWholeOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemLoadOp',
        exec_template_base='VMemLoadWhole')
}};

def template VMemLoadWholeExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        Fault fault = NoFault;
        Addr EA;

        size_t NFIELDS = NF + 1;
        size_t elem_per_reg = TheISA::VLEN / mem_bits(WIDTH);
        for (size_t greg_idx = 0; greg_idx < NFIELDS; greg_idx++) {
            %(op_decl)s; %(op_rd)s; %(ea_code)s;

            auto offset = TheISA::vlenb * greg_idx;
            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            fault = readMemAtomic(xc, EA + offset, Mem.as<uint8_t>(),
                TheISA::vlenb, this->memAccessFlags, byte_enable);

            if (fault != NoFault) return fault;

            for (size_t i = 0; i < elem_per_reg; i++) {
                %(memacc_code)s;
            }
            %(op_wb)s;
        }

        return fault;
    }
}};

def template VMemLoadWholeInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        auto nums = reg_group_nums(vtype);
        Addr EA;

        %(op_src_decl)s;
        %(op_rd)s;
        %(ea_code)s;

        const std::vector<bool> byte_enable(sizeof(Mem) * nums, true);
        return initiateMemRead(xc, EA, sizeof(Mem) * nums,
            this->memAccessFlags, byte_enable);
    }
}};

def template VMemLoadWholeCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        size_t NFIELDS = NF + 1;
        size_t elem_per_reg = TheISA::VLEN / mem_bits(WIDTH);
        for (size_t greg_idx = 0; greg_idx < NFIELDS; greg_idx++) {
            %(op_decl)s; %(op_rd)s;

            auto offset = TheISA::vlenb * greg_idx;
            memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>() + offset,
                   TheISA::vlenb);
            for (size_t i = 0; i < elem_per_reg; i++) {
                %(memacc_code)s;
            }
            %(op_wb)s;
        }

        return NoFault;
    }
}};

def format VMemLoadMaskOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemLoadOp',
        exec_template_base='VMemLoadMask')
}};

def template VMemLoadMaskExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        Fault fault = NoFault;
        Addr EA;
        size_t vl = tc->readMiscReg(MISCREG_VL);
        auto bytes_to_read = (size_t)ceil((float)vl / 8.0);
        size_t greg_idx = 0;

        %(op_decl)s; %(op_rd)s; %(ea_code)s;

        const std::vector<bool> byte_enable(bytes_to_read, true);
        fault = readMemAtomic(xc, EA, Mem.as<uint8_t>(),
            bytes_to_read, this->memAccessFlags, byte_enable);
        if (fault != NoFault) return fault;

        for (size_t i = 0; i < bytes_to_read; i++) {
            %(memacc_code)s;
        }
        %(op_wb)s;
        return fault;
    }
}};

def template VMemLoadMaskInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);
        auto nums = reg_group_nums(vtype);
        Addr EA;

        %(op_src_decl)s;
        %(op_rd)s;
        %(ea_code)s;

        const std::vector<bool> byte_enable(sizeof(Mem) * nums, true);
        return initiateMemRead(xc, EA, sizeof(Mem) * nums,
            this->memAccessFlags, byte_enable);
    }
}};

def template VMemLoadMaskCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        Fault fault = NoFault;
        size_t vl = tc->readMiscReg(MISCREG_VL);
        auto bytes_to_read = (size_t)ceil((float)vl / 8.0);
        size_t greg_idx = 0;

        %(op_decl)s; %(op_rd)s;

        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), TheISA::vlenb);
        for (size_t i = 0; i < bytes_to_read; i++) {
            %(memacc_code)s;
        }
        %(op_wb)s;
        return fault;
    }
}};

def format VMemStoreOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemStoreOp',
        exec_template_base='VMemStore')
}};

def template VMemStoreExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        Addr EA;
        Fault fault = NoFault;

        size_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);

        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;
        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            %(op_decl)s; %(op_rd)s; %(ea_code)s;

            auto offset = TheISA::vlenb * greg_idx;
            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            fault = readMemAtomic(xc, EA + offset, Mem.as<uint8_t>(),
                TheISA::vlenb, this->memAccessFlags, byte_enable);
            if (fault != NoFault) return fault;

            %(memacc_code)s;
            %(op_wb)s;

            fault = writeMemAtomic(xc, Mem.as<uint8_t>(), EA + offset,
                TheISA::vlenb, this->memAccessFlags, nullptr, byte_enable);
            if (fault != NoFault) return fault;
        }

        return fault;
    }
}};

def template VMemStoreInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        CHECK_VILL;

        Addr EA;
        Fault fault = NoFault;

        size_t vl = tc->readMiscReg(MISCREG_VL);
        uint64_t vtype = tc->readMiscReg(MISCREG_VTYPE);

        const TheISA::vreg_t& tmp_v0 = tc->readVecReg(RegId(VecRegClass, 0));
        [[maybe_unused]] auto v0 = tmp_v0.as<uint8_t>();

        auto nums = reg_group_nums(vtype);
        auto vlmax_per_reg = VLMAX(vtype, true);
        size_t greg_idx = 0, ei = 0;
        for (; greg_idx < nums; greg_idx++) {
            if (ei >= vl) break;
            %(op_decl)s; %(op_rd)s; %(ea_code)s;

            auto offset = TheISA::vlenb * greg_idx;
            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            fault = readMemAtomic(xc, EA + offset, Mem.as<uint8_t>(),
                TheISA::vlenb, this->memAccessFlags, byte_enable);
            if (fault != NoFault) return fault;

            %(memacc_code)s;
            %(op_wb)s;

            fault = writeMemTiming(xc, Mem.as<uint8_t>(), EA + offset,
                TheISA::vlenb, this->memAccessFlags, nullptr, byte_enable);
            if (fault != NoFault) return fault;
        }

        return fault;
    }
}};

def template VMemStoreCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        return NoFault;
    }
}};

def format VMemStoreWholeOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemStoreOp',
        exec_template_base='VMemStoreWhole')
}};

def template VMemStoreWholeExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        Fault fault = NoFault;
        Addr EA;

        size_t NFIELDS = NF + 1;
        for (size_t greg_idx = 0; greg_idx < NFIELDS; greg_idx++) {
            %(op_decl)s; %(op_rd)s; %(ea_code)s;
            for (size_t i = 0; i < TheISA::vlenb; i++) {
                %(memacc_code)s;
            }
            %(op_wb)s;

            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            auto offset = TheISA::vlenb * greg_idx;
            fault = writeMemAtomic(xc, Mem.as<uint8_t>(),
                EA + offset, TheISA::vlenb,
                this->memAccessFlags, nullptr, byte_enable);
            if (fault != NoFault) return fault;
        }

        return fault;
    }
}};

def template VMemStoreWholeInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        Fault fault = NoFault;
        Addr EA;

        size_t NFIELDS = NF + 1;
        for (size_t greg_idx = 0; greg_idx < NFIELDS; greg_idx++) {
            %(op_decl)s; %(op_rd)s; %(ea_code)s;
            for (size_t i = 0; i < TheISA::NumVecElemPerVecReg; i++) {
                %(memacc_code)s;
            }
            %(op_wb)s;

            const std::vector<bool> byte_enable(TheISA::vlenb, true);
            auto offset = TheISA::vlenb * greg_idx;
            fault = writeMemTiming(xc, Mem.as<uint8_t>(),
                EA + offset, TheISA::vlenb,
                this->memAccessFlags, nullptr, byte_enable);
            if (fault != NoFault) return fault;
        }

        return fault;
    }
}};

def template VMemStoreWholeCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        return NoFault;
    }
}};

def format VMemStoreMaskOp (
    memacc_code, ea_code={{EA = Rs1}},
    mem_flags=[], inst_flags=[]
) {{
    (header_output, decoder_output, decode_block, exec_output) = \
        VMemLoadStoreOpBase(name, Name, memacc_code, ea_code,
        mem_flags, inst_flags, 'VMemStoreOp',
        exec_template_base='VMemStoreMask')
}};

def template VMemStoreMaskExecute {{
    Fault %(class_name)s::execute(
        ExecContext *xc, Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        Fault fault = NoFault;
        Addr EA;
        size_t vl = tc->readMiscReg(MISCREG_VL);
        auto bytes_to_read = (size_t)ceil((float)vl / 8.0);
        size_t greg_idx = 0;

        %(op_decl)s; %(op_rd)s; %(ea_code)s;
        for (size_t i = 0; i < bytes_to_read; i++) {
            %(memacc_code)s;
        }
        %(op_wb)s;

        const std::vector<bool> byte_enable(TheISA::vlenb, true);
        fault = writeMemAtomic(xc, Mem.as<uint8_t>(), EA, TheISA::vlenb,
            this->memAccessFlags, nullptr, byte_enable);
        return fault;
    }
}};

def template VMemStoreMaskInitiateAcc {{
    Fault %(class_name)s::initiateAcc(ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        auto tc = xc->tcBase();
        Fault fault = NoFault;
        Addr EA;
        size_t vl = tc->readMiscReg(MISCREG_VL);
        auto bytes_to_read = (size_t)ceil((float)vl / 8.0);
        size_t greg_idx = 0;

        %(op_decl)s; %(op_rd)s; %(ea_code)s;
        for (size_t i = 0; i < bytes_to_read; i++) {
            %(memacc_code)s;
        }
        %(op_wb)s;

        const std::vector<bool> byte_enable(TheISA::vlenb, true);
        fault = writeMemTiming(xc, Mem.as<uint8_t>(), EA, TheISA::vlenb,
            this->memAccessFlags, nullptr, byte_enable);
        return fault;
    }
}};

def template VMemStoreMaskCompleteAcc {{
    Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
        Trace::InstRecord *traceData) const
    {
        return NoFault;
    }
}};